+++
title = '文章标题'
date = 2024-04-26T22:32:18+08:00
draft = 大家好，今天跟大家分享一下我这几天对大语言模型的本地部署做的一些探索，我想将大模型与本地知识库构建连接，并且具有一定的可控性，所以在过程中碰到的一些大小不一的问题，想做个视频分享给有同样需求的友友。话不多说直接上才艺，一开始是准备用开源的ollama部署本地大模型，由于我的windows用户名文件夹带了中文，导致ollama下载的模型无法运行，很是尴尬。所以我在我的1050ti的windows老电脑上试着使用ollama，但是每次运行都是cpu内存的使用率和占有率非常高，显存使用率很低，还出现对话到一半直接罢工的情况，所以只能转战显卡可控性更高，并且可以忽略我windows中文用户名的开源平台LM Studio来进行大模型的本地部署（这我也是使用了LM studio后才知道），但LM studio也有缺点，就是相对于ollama，LM studio的模型获取门槛更高，在使用了魔法后还不能在平台顺畅下载模型后，只能转向网站Hugging Face手动下载模型了，而对于LM studio平台可以直接使用的模型格式GGUF，有些模型并不能直接下载到这种格式的文件，当然还是有办法，我在这里就不详细说了，所以只使用在Hugging Face上可以下载到GGUF格式的模型。当我们想选择好想要下载的模型后，又出现一个问题，模型大小以及硬件支持，大多数模型对内存和显存都有一定的要求，宽泛来说，一般7B的模型，要求都是8G内存和8G显存以上，所以在Hugging Face网站上进行模型选择的时候，可以点进模型介绍去看看模型的基本信息，有些介绍中含有最大内存要求，防止模型下载了却不能使用。GGUF格式的模型文件下载过后，要放在LM studio的本地模型文件夹（可以任意设置）下，需要注意的一点是，下载好的模型文件要放在本地模型文件夹的孙目录文件夹下，也就是子文件夹的子文件夹，模型文件只能放在孙文件夹里面，文件名任意。好的，如果将模型文件放置正确，就可以直接在LM studio提供的对话页面进行模型选择，然后进行对话了，本地部署模型成功，撒花！！
+++
